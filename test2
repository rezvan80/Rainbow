# -*- coding: utf-8 -*-
from __future__ import division
import os
import plotly
from plotly.graph_objs import Scatter
from plotly.graph_objs.scatter import Line
import torch
import osmnx as ox
import networkx as nx
import numpy as np
import random
import torch
def subtract_from_list(list_of_lists, value):
    for sublist in list_of_lists:
        value1 = value
        i = 0
        while value1 > 0 and i < len(sublist):
            if sublist[i] >= value1:
                sublist[i] -= value1
                value1 = 0
            else:
                value1 -= sublist[i]
                sublist[i] = 0
                i += 1
        # Now remove all zero values from this sublist
        # Use list comprehension for in-place update
        sublist[:] = [x for x in sublist if x != 0]
    return list_of_lists
import math
def fcc_encoder(arrs, ch_time , values):
    fcc=[]
    arrs=arrs.tolist()
    ch_time=ch_time.tolist()
    values=values.tolist()
    for arr, value in zip(arrs, values):
      sorted_arr = list(enumerate(arr))
      sorted_arr = [item for item in sorted_arr if (item[1] is not None and item[1] < value ) ]
      sorted_arr.sort(key=lambda item: item[1], reverse=True)
      indices_to_save=[]

      s=value
      for i in range(0 , len(sorted_arr)-1):

            diff=s-sorted_arr[i][1]
            s=sorted_arr[i][1]
            if diff>ch_time[sorted_arr[i][0]]:

              break

            indices_to_save.append(sorted_arr[i][0])

      fcc.append(
      sum([ch_time[i] for i in indices_to_save]) + arr[indices_to_save[-1]] - value
      if indices_to_save
      else 0
      )
    fcc = nn.functional.softmax(torch.tensor(fcc).float()  , dim=0)
    return fcc
# Getting a street network for Manhattan, NYC
G = ox.graph_from_place("Piedmont, California, USA", network_type="drive")
# Download the street network within this bbox
G = ox.graph_from_bbox([ -122.2224914, 37.8222894,-122.2324914 , 37.8322894 ], network_type="drive")

#G = ox.graph_from_place("Oakland, California, USA", network_type="drive")
nodes = list(G.nodes())
reindex_mapping = {node: i for i, node in enumerate(nodes)}
G = nx.relabel_nodes(G, reindex_mapping)
nodes = list(G.nodes())
charging_station_nodes= random.sample(nodes, 3)

remaining_nodes = list(set(nodes) - set(charging_station_nodes))
start_node= random.sample(remaining_nodes, 20)
node_color = []

for node in G.nodes:
      if node in charging_station_nodes:  # Start node
          node_color.append('green')  # Start node in green
      elif node == start_node[0]:  # End node
          node_color.append('red')
      else:
          node_color.append('white')  # Other nodes in blue
#shortest_path = nx.shortest_path(G, source=start_node[0], target=charging_station_nodes[0], weight="length")
# Plot the graph and the shortest path
ox.plot_graph(G, node_color=node_color , node_size=30, figsize=(10, 8))
G = G.to_undirected()
from node2vec import Node2Vec

# Apply Node2Vec for graph embedding
node2vec = Node2Vec(G, dimensions=16, walk_length=10, num_walks=100, workers=4)

# Train the Node2Vec model
model = node2vec.fit(window=5, min_count=1)
import gymnasium as gym
from gymnasium  import spaces
from collections import deque
j=0
class charging_stationEnv5(gym.Env):
    def __init__(self, graph , charging_station_nodes , n_ev ):
        super(charging_stationEnv5, self).__init__()
        self.graph = graph
        self.j=0
        self.station_arr=np.array([[None]*n_ev ]*3)
        self.station_ch=list([[0] , [0] ,[0]])
        self.charging_station_nodes = [9 , 19 , 13]

        self.desierd_soc=list(np.random.uniform(0.6, 0.8,n_ev ))
        self.current_soc=list(np.random.uniform(0.4, 0.6,n_ev ))
        self.current_node= random.sample(list(set(list(self.graph.nodes())) - set(self.charging_station_nodes)),n_ev )
        #self.desierd_soc=[0.69327472, 0.71491305, 0.70253482, 0.64029819, 0.7861011 ,
        #0.72377404, 0.66907586, 0.68648428, 0.72050732, 0.70266698,
        #0.73624628, 0.60573625, 0.79150536, 0.77703977, 0.75890208,
        #0.63177314, 0.74209745, 0.66636456, 0.77107797, 0.62668404]
        #self.current_soc=[0.51684445, 0.50067023, 0.53853516, 0.42557636, 0.46108417,
        #0.44697881, 0.55068407, 0.43465217, 0.47451146, 0.5875858 ,
        #0.5525329 , 0.43837829, 0.57131378, 0.47084906, 0.45499856,
        #0.54569187, 0.53936948, 0.52403585, 0.47374796, 0.59744536]
        #self.current_node=[7, 38, 33, 21, 0, 20, 25, 2, 12, 45, 11, 48, 15, 49, 40, 44, 52, 18, 27, 28]
         #self.path = [self.start_node]  # Initialize path tracker
        # Initialize path tracker
        self.charging_time=np.array([0]*n_ev)
        self.fcc= [deque([0]*30 , maxlen=30) for _ in range(n_ev )]
        self.label= [deque([0]*640 , maxlen=640) for _ in range(n_ev )]
        self.arrival_time=np.array([0]*3)
        self.num_envs=1
        self.test=False
        self.iteration= 0
        self.reward=[0]*n_ev
        self.state=[[0]*17]*n_ev
        self.done=[False]*n_ev
        self.average_reward=[0]
        # Define action and observation spaces
        self.action_space =spaces.Discrete(3)  # One action per node
        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(17,), dtype=np.float32)
        self.travel_times=[0]*n_ev
        self.action=[0]*n_ev

        #self.average_reward=0
        self.average_distance=[0]
        self.distance=[0]*n_ev
        self.node=[0]*len(self.graph)
        #self.path=[[144], [290], [199], [193], [328], [302], [135], [274], [131], [35], [118], [29], [261], [104], [308], [207], [74], [86], [128], [211]]
    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.distance[self.j]=0
        """ Reset the environment to the initial state (start node). """
        self.current_soc[self.j]=[0.51684445, 0.50067023, 0.53853516, 0.42557636, 0.46108417,
        0.44697881, 0.55068407, 0.43465217, 0.47451146, 0.5875858 ,
        0.5525329 , 0.43837829, 0.57131378, 0.47084906, 0.45499856,
        0.54569187, 0.53936948, 0.52403585, 0.47374796, 0.59744536][self.j]
        self.current_node[self.j]=[7, 38, 33, 21, 0, 20, 25, 2, 12, 45, 11, 48, 15, 49, 40, 44, 52, 18, 27, 28][self.j]
        self.desierd_soc[self.j]=list(np.random.uniform(0.6, 0.8,1 ))[0]
        self.current_soc[self.j]=list(np.random.uniform(0.4, 0.6,1 ))[0]
        self.current_node[self.j]= random.sample(list(set(list(self.graph.nodes())) - set(self.charging_station_nodes)),1 )[0]

        #self.path = [self.start_node]  # Reset path tracker
        self.state[self.j]=np.concatenate((model.wv[str(self.current_node[self.j])],model.wv[str(self.charging_station_nodes[0])],model.wv[str(self.charging_station_nodes[1])],model.wv[str(self.charging_station_nodes[2])] ,np.array([model.wv[str(node_id)] for node_id in self.current_node ]).reshape(-1)) ,axis=0)
        #self.state[self.j]=np.array(charging_time(self.station_arr , self.charging_time))

        self.state[self.j]=np.concatenate((model.wv[str(self.current_node[self.j])],model.wv[str(self.charging_station_nodes[0])],model.wv[str(self.charging_station_nodes[1])],model.wv[str(self.charging_station_nodes[2])] ,np.array([model.wv[str(node_id)] for node_id in self.current_node ]).reshape(-1) ,np.array([self.desierd_soc[self.j]-self.current_soc[self.j]]), np.array(self.desierd_soc)-np.array(self.current_soc) ) ,axis=0)
        #self.state[self.j]=model.wv[str(self.current_node[self.j])]
        self.state[self.j]=np.concatenate((model.wv[str(self.current_node[self.j])],np.array([self.current_soc[self.j]-self.desierd_soc[self.j]])  )  , axis=0)
        #
        #self.state[self.j]=np.array(np.concatenate((model.wv[str(self.current_node[self.j])] ) , axis=0))
        #self.state[self.j]=np.array(self.fcc[self.j])
        #self.state[self.j]=np.concatenate((model.wv[str(self.current_node[self.j])] , np.where(self.station_arr == None, 0, self.station_arr).reshape(-1)) , axis=0)
        self.distance[self.j]=0
        return self.state[self.j] , {}

    def step(self , action ):


        self.done[self.j] = False
        shortest_path = nx.shortest_path(self.graph, source=self.current_node[self.j], target=self.charging_station_nodes[action], weight="length")
        """ Take a step in the environment with the given action. """

        self.station_arr[int(action)][int(self.j)]= nx.shortest_path_length(G, source=self.current_node[self.j], target=self.charging_station_nodes[action], weight="length")

        for row_idx in range(len(self.station_arr)):

               if row_idx != int(action):

                    self.station_arr[row_idx][self.j] = None

        distance=self.graph.get_edge_data(shortest_path[0] , shortest_path[1])[0]['length']
        self.travel_times[self.j] =distance
        self.current_soc[self.j]=self.current_soc[self.j]-distance*0.0001
        self.charging_time[self.j]=(self.desierd_soc[self.j]-self.current_soc[self.j])/0.002
        self.iteration += 1
        self.reward[self.j] = -0.01*distance

        self.distance[self.j]+=distance
        min_val = min(self.travel_times)
        self.station_ch=subtract_from_list(self.station_ch,min(self.travel_times)/10)
        self.label[self.j].extend(model.wv[str(self.current_node[self.j])])
        if self.node[self.current_node[self.j]] > 0:
           self.node[self.current_node[self.j]] -=1
        self.current_node[self.j] = shortest_path[1]
        self.node[self.current_node[self.j]] +=1
        #reward=1
        #done = self.current_node == self.goal_node
        # Plot the graph and the shortest path
        #self.plot_path()

        #if self.current_node[self.j] in self.charging_station_nodes:

        #if self.iteration == 4000:
              #self.station_ch=np.array([[0]]*3)
              #self.iteration= 0



        self.arrival_time = np.array([
              nx.shortest_path_length(
                    G, source=self.current_node[self.j], target=charging_station, weight="length"
                )
                for charging_station in self.charging_station_nodes
            ])

        #self.fcc[self.j].extend(fcc_encoder(self.station_arr , self.charging_time , self.arrival_time))

              #reward=-np.sum(fcc_encoder(self.station_arr , self.charging_time , self.arrival_time))

        #if self.current_node[self.j] in self.charging_station_nodes:

        if self.current_node[self.j] in self.charging_station_nodes:
           self.average_reward.append(np.sum(self.station_ch[action]))
           ch=(self.desierd_soc[self.j]-self.current_soc[self.j])/0.001
           #self.reward[self.j]=10
           self.station_ch[action].append((self.desierd_soc[self.j]-self.current_soc[self.j])/0.002)
           self.reward[self.j]=-0.01*np.sum(self.station_ch[action])

           #self.reward[self.j]=10
           self.average_distance.append(self.distance[self.j])
           self.node[self.current_node[self.j]]-=1

           self.done[self.j] = True
           #action  = self.charging_station_nodes.index(self.current_node[self.j])  # Large reward for reaching the goal

        #self.reward[self.j]=100/(1+np.sum(self.station_ch[action]))
        #st_node[self.j])],model.wv[str(self.charging_station_nodes[0])],model.wv[str(self.charging_station_nodes[1])],model.wv[str(self.charging_station_nodes[2])] ,np.array([model.wv[str(node_id)] for node_id in self.current_node ]).reshape(-elf.reward[self.j]=-np.sum(charging_time(self.station_arr , self.charging_time ))/100
        #self.reward[self.j]=-np.sum(charging_time(self.station_arr , self.charging_time ))/100


        self.state[self.j]=np.concatenate((model.wv[str(self.current_node[self.j])],model.wv[str(self.charging_station_nodes[0])],model.wv[str(self.charging_station_nodes[1])],model.wv[str(self.charging_station_nodes[2])] ,np.array([model.wv[str(node_id)] for node_id in self.current_node ]).reshape(-1) , np.array([self.desierd_soc[self.j]-self.current_soc[self.j]]) , np.array(self.desierd_soc)-np.array(self.current_soc) ) ,axis=0)
        #self.state[self.j]=model.wv[str(self.current_node[self.j])]
        #self.state[self.j]=np.array(charging_time(self.station_arr , self.charging_time))
        self.state[self.j]=np.concatenate((model.wv[str(self.current_node[self.j])],np.array([self.desierd_soc[self.j]-self.current_soc[self.j]])  ) , axis=0)

        #self.state[self.j]=np.array(np.concatenate((model.wv[str(self.current_node[self.j])],np.array([self.current_soc[self.j]]),np.array([self.desierd_soc[self.j]])) , axis=0))
        #self.state[self.j]=np.concatenate((model.wv[str(self.current_node[self.j])] , np.where(self.station_arr == None, 0, self.station_arr).reshape(-1)) ,axis=0)

        return self.state[self.j] ,self.reward[self.j] ,self.done[self.j] ,  self.done[self.j] , {}


    def plot_path(self):
        """Visualizes the graph and highlights the visited path using ox.plot_graph."""
        # Add x and y coordinates as node attributes for OSMnx plotting

        # Highlight nodes in the path with distinct colors for start, end, and others
        node_color = []
        for node in self.graph.nodes:
            if node == self.current_node[self.j]:  # End node
                node_color.append('blue')
            elif node in self.charging_station_nodes:  # Start node
                node_color.append('green')  # Start node in green
            elif node in self.current_node:  # End node
                node_color.append('red')  # End node in red
            elif node == self.current_node[self.j]:  # End node
                node_color.append('blue')
            #elif node == self.start_node2:  # End node
                #node_color.append('red')  # End node in red

            else:
                node_color.append('white')  # Other nodes in blue

            # Plot the graph using ox.plot_graph
        ox.plot_graph(G, node_color=node_color , node_size=30, figsize=(10, 8))

# Test DQN
def test(args, T, dqn, val_mem, metrics, results_dir, evaluate=False):
  n_ev=20
  env=charging_stationEnv5(G, charging_station_nodes , n_ev)
  state=[None]*n_ev
  action=[None]*n_ev
  done=[None]*n_ev
  reward=[None]*n_ev
  reward_sum=[None]*n_ev
  metrics['steps'].append(T)
  T_rewards, T_Qs = [], []

  # Test performance over several episodes
  
  for T in range(args.evaluation_episodes):
    
    reward_sum ,done = 0 , [False]*n_ev
    env.done = [False]*n_ev
    env.station_ch=list([[0] , [0] ,[0]])
    for i in range(20):
      env.j=i
      state[i] , _ = env.reset()
      state[i] = torch.tensor(state[i] ,  dtype = torch.float32 , device = 'cpu')
    while (all(done) == False):
       
        
      for i in range(20):
       env.j=i
       if done[i] == False: 
          action[i] = dqn[i].act_e_greedy(state[i])  # Choose an action ε-greedily
        
          state[i], reward[i], done[i] , _ , _ = env.step(action[i])  # Step
          state[i] = torch.tensor(state[i], dtype=torch.float32, device='cpu')
          reward_sum += reward[i]
        
          if args.render:
            env.render()

        
    T_rewards.append(reward_sum)
          
        

    # Test Q-values over validation memory
    # Iterate over valid states
    for i in range(n_ev):
      for sta in val_mem[i]:
        
        T_Qs.append(dqn[i].evaluate_q(sta))
  
  avg_reward, avg_Q = sum(T_rewards) / len(T_rewards), sum(T_Qs) / len(T_Qs)
  if not evaluate:
      # Save model parameters if improved
      if avg_reward > metrics['best_avg_reward']:
          metrics['best_avg_reward'] = avg_reward
          for i in range(n_ev):
            dqn[i].save(results_dir)

          # Append to results and save metrics
          metrics['rewards'].append(T_rewards)
          metrics['Qs'].append(T_Qs)
          torch.save(metrics, os.path.join(results_dir, 'metrics.pth'))
 
          # Plot
          #_plot_line(metrics['steps'], metrics['rewards'], 'Reward', path=results_dir)
          #_plot_line(metrics['steps'], metrics['Qs'], 'Q', path=results_dir)

  # Return average reward and Q-value
  return avg_reward, avg_Q


# Plots min, max and mean + standard deviation bars of a population over time
def _plot_line(xs, ys_population, title, path=''):
  max_colour, mean_colour, std_colour, transparent = 'rgb(0, 132, 180)', 'rgb(0, 172, 237)', 'rgba(29, 202, 255, 0.2)', 'rgba(0, 0, 0, 0)'

  ys = torch.tensor(ys_population, dtype=torch.float32)
  ys_min, ys_max, ys_mean, ys_std = ys.min(1)[0].squeeze(), ys.max(1)[0].squeeze(), ys.mean(1).squeeze(), ys.std(1).squeeze()
  ys_upper, ys_lower = ys_mean + ys_std, ys_mean - ys_std

  trace_max = Scatter(x=xs, y=ys_max.numpy(), line=Line(color=max_colour, dash='dash'), name='Max')
  trace_upper = Scatter(x=xs, y=ys_upper.numpy(), line=Line(color=transparent), name='+1 Std. Dev.', showlegend=False)
  trace_mean = Scatter(x=xs, y=ys_mean.numpy(), fill='tonexty', fillcolor=std_colour, line=Line(color=mean_colour), name='Mean')
  trace_lower = Scatter(x=xs, y=ys_lower.numpy(), fill='tonexty', fillcolor=std_colour, line=Line(color=transparent), name='-1 Std. Dev.', showlegend=False)
  trace_min = Scatter(x=xs, y=ys_min.numpy(), line=Line(color=max_colour, dash='dash'), name='Min')

  plotly.offline.plot({
    'data': [trace_upper, trace_mean, trace_lower, trace_min, trace_max],
    'layout': dict(title=title, xaxis={'title': 'Step'}, yaxis={'title': title})
  }, filename=os.path.join(path, title + '.html'), auto_open=False)
